#!/usr/bin/env python3
"""
é©—è­‰è½‰æ›å¾Œçš„ dataset æ˜¯å¦å¯ä»¥è¢« train/inference æ­£ç¢ºè®€å–
"""

import sys
from pathlib import Path
from datasets import load_dataset

# Add CLD tools to path
sys.path.insert(0, str(Path(__file__).parent))

from tools.dlcv_dataset import DLCVLayoutDataset, collate_fn
from torch.utils.data import DataLoader

def verify_converted_dataset(converted_data_dir: str, num_samples: int = 3):
    """
    é©—è­‰è½‰æ›å¾Œçš„ dataset
    
    Args:
        converted_data_dir: è½‰æ›å¾Œçš„ dataset æ ¹ç›®éŒ„
        num_samples: è¦æ¸¬è©¦çš„æ¨£æœ¬æ•¸
    """
    print("="*60)
    print("é©—è­‰è½‰æ›å¾Œçš„ Dataset")
    print("="*60)
    
    # === 1. æª¢æŸ¥ parquet æ–‡ä»¶æ ¼å¼ ===
    print("\n[æ­¥é©Ÿ 1] æª¢æŸ¥ Parquet æ–‡ä»¶æ ¼å¼...")
    
    data_dir = Path(converted_data_dir)
    parquet_dir = data_dir / "snapshots" / "snapshot_1" / "data"
    
    if not parquet_dir.exists():
        parquet_dir = data_dir / "data"
    
    parquet_files = list(parquet_dir.glob("*.parquet"))
    if not parquet_files:
        print(f"âŒ æ‰¾ä¸åˆ° parquet æ–‡ä»¶åœ¨ {parquet_dir}")
        return False
    
    print(f"âœ“ æ‰¾åˆ° {len(parquet_files)} å€‹ parquet æ–‡ä»¶")
    
    # è¼‰å…¥ç¬¬ä¸€å€‹æ–‡ä»¶æª¢æŸ¥
    first_parquet = parquet_files[0]
    print(f"  æª¢æŸ¥: {first_parquet.name}")
    
    ds = load_dataset('parquet', data_files=str(first_parquet))['train']
    print(f"  âœ“ è¼‰å…¥æˆåŠŸ: {len(ds)} å€‹æ¨£æœ¬")
    
    # === 2. æª¢æŸ¥å¿…è¦æ¬„ä½ ===
    print("\n[æ­¥é©Ÿ 2] æª¢æŸ¥å¿…è¦æ¬„ä½...")
    
    sample = ds[0]
    required_fields = {
        'preview': str,
        'title': str,
        'left': list,
        'top': list,
        'width': list,
        'height': list,
        'length': int,
        'image': list,
        'canvas_width': (int, type(None)),
        'canvas_height': (int, type(None)),
        'type': (list, type(None)),
    }
    
    all_good = True
    for field, expected_type in required_fields.items():
        if field not in sample:
            print(f"  âŒ ç¼ºå°‘æ¬„ä½: {field}")
            all_good = False
        else:
            value = sample[field]
            if isinstance(expected_type, tuple):
                if not isinstance(value, expected_type):
                    print(f"  âš ï¸  {field}: {type(value)} (æœŸæœ› {expected_type})")
            else:
                if not isinstance(value, expected_type):
                    print(f"  âŒ {field}: {type(value)} (æœŸæœ› {expected_type})")
                    all_good = False
                else:
                    print(f"  âœ“ {field}: {type(value).__name__}")
    
    if not all_good:
        print("\nâŒ æ¬„ä½æª¢æŸ¥å¤±æ•—ï¼")
        return False
    
    # === 3. æª¢æŸ¥è³‡æ–™å…§å®¹ ===
    print("\n[æ­¥é©Ÿ 3] æª¢æŸ¥è³‡æ–™å…§å®¹...")
    
    print(f"  preview: {sample['preview']}")
    print(f"  title: {sample['title'][:80]}...")
    print(f"  length: {sample['length']}")
    print(f"  left: {sample['left'][:3]}... (length={len(sample['left'])})")
    print(f"  image: {len(sample['image'])} layers")
    
    # æª¢æŸ¥åœ–ç‰‡è·¯å¾‘æ˜¯å¦å­˜åœ¨
    import os
    preview_path = sample['preview']
    if isinstance(preview_path, str):
        if os.path.exists(preview_path):
            print(f"  âœ“ preview åœ–ç‰‡å­˜åœ¨")
        else:
            print(f"  âŒ preview åœ–ç‰‡ä¸å­˜åœ¨: {preview_path}")
            all_good = False
    
    # æª¢æŸ¥ layer åœ–ç‰‡
    layer_paths = sample['image']
    existing_layers = 0
    for i, layer_path in enumerate(layer_paths):
        if layer_path and isinstance(layer_path, str) and os.path.exists(layer_path):
            existing_layers += 1
    
    print(f"  âœ“ {existing_layers}/{len(layer_paths)} layer åœ–ç‰‡å­˜åœ¨")
    
    # === 4. æ¸¬è©¦ç”¨ DLCVLayoutDataset è¼‰å…¥ ===
    print("\n[æ­¥é©Ÿ 4] æ¸¬è©¦ç”¨ DLCVLayoutDataset è¼‰å…¥...")
    
    try:
        dataset = DLCVLayoutDataset(
            data_dir=str(converted_data_dir),
            split="train",
            caption_mapping_path=None,  # captions å·²ç¶“åœ¨ parquet ä¸­
            enable_debug=False
        )
        print(f"  âœ“ DLCVLayoutDataset è¼‰å…¥æˆåŠŸ: {len(dataset)} å€‹æ¨£æœ¬")
    except Exception as e:
        print(f"  âŒ DLCVLayoutDataset è¼‰å…¥å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # === 5. æ¸¬è©¦ DataLoader ===
    print("\n[æ­¥é©Ÿ 5] æ¸¬è©¦ DataLoader...")
    
    try:
        loader = DataLoader(
            dataset,
            batch_size=1,
            shuffle=False,
            num_workers=0,
            collate_fn=collate_fn
        )
        
        # æ¸¬è©¦è®€å–å¹¾å€‹æ¨£æœ¬
        for i, batch in enumerate(loader):
            if i >= num_samples:
                break
            
            print(f"\n  æ¨£æœ¬ {i}:")
            print(f"    caption: {batch['caption'][:60]}...")
            print(f"    pixel_RGBA shape: {batch['pixel_RGBA'].shape}")
            print(f"    pixel_RGB shape: {batch['pixel_RGB'].shape}")
            print(f"    layout: {len(batch['layout'])} layers")
            print(f"    height: {batch['height']}, width: {batch['width']}")
        
        print(f"\n  âœ“ DataLoader æ¸¬è©¦æˆåŠŸï¼")
        
    except Exception as e:
        print(f"  âŒ DataLoader æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # === 6. æœ€çµ‚çµæœ ===
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰é©—è­‰é€šéï¼")
    print("="*60)
    print("\nğŸ’¡ é€™å€‹ dataset å¯ä»¥ç›´æ¥ç”¨æ–¼:")
    print(f"  - Training: data_dir=\"{converted_data_dir}\"")
    print(f"  - Inference: data_dir=\"{converted_data_dir}\"")
    print(f"  - ä¸éœ€è¦é¡å¤–çš„ caption_mappingï¼ˆcaptions å·²åœ¨ parquet ä¸­ï¼‰")
    
    return True


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="é©—è­‰è½‰æ›å¾Œçš„ dataset")
    parser.add_argument(
        "--data_dir",
        type=str,
        required=True,
        help="è½‰æ›å¾Œçš„ dataset æ ¹ç›®éŒ„"
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=3,
        help="è¦æ¸¬è©¦çš„æ¨£æœ¬æ•¸ï¼ˆé è¨­: 3ï¼‰"
    )
    
    args = parser.parse_args()
    
    success = verify_converted_dataset(
        converted_data_dir=args.data_dir,
        num_samples=args.num_samples,
    )
    
    sys.exit(0 if success else 1)
