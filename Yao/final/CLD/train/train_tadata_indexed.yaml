# 訓練配置：使用方案 B (Index-based TAData + LLaVA captions)
# 不需要轉換數據集，直接使用 TAData + caption.json

seed: 42
max_layer_num: 52
num_inference_steps: 28

lora_rank: 64
lora_alpha: 64
lora_dropout: 0

max_steps: 200000
log_every: 1000
save_every: 1000
max_sequence_length: 512
cfg: 4.0
accum_steps: 4
optimizer: Prodigy

adapter_scale: 0.9

# ⭐ 方案 B 配置：使用 TAData + caption.json
use_indexed_dataset: true  # 啟用 index-based caption matching
data_dir: "/workspace/dataset/DLCV_dataset/data"  # TAData 目錄
caption_mapping: "/workspace/ShuoIsAllYouNeed/Yao/final/CLD/caption_llava16_final.json"  # LLaVA captions

# Debug 設定
enable_dataset_debug: true  # 顯示前 3 個樣本的詳細載入資訊

# 模型路徑（根據你的環境調整）
artplus_lora_dir: "/workspace/ShuoIsAllYouNeed/Yao/final/CLD/ckpt/prism_ft_LoRA"
resume_from: "/workspace/ShuoIsAllYouNeed/Yao/final/CLD/ckpt/decouple_LoRA"

pretrained_model_name_or_path: "flux_model"
pretrained_adapter_path: "Path_to_pretrained_FLUX_adapter"
pretrained_lora_dir: "ckpt/pre_trained_LoRA"
output_dir: "FT_on_TAData_ckpt"

