# Inference 配置：使用方案 B (Index-based TAData + LLaVA captions)
# 對應 training 配置：train_tadata_indexed.yaml

seed: 42
max_layer_num: 52

# ⭐ 方案 B 配置：使用 TAData + caption.json
use_indexed_dataset: true  # Inference 使用 indexed dataset（推薦）
data_dir: "/workspace/dataset/DLCV_dataset"  # TAData 目錄
caption_json: "/workspace/ShuoIsAllYouNeed/Yao/final/CLD/caption_llava16_final.json"  # LLaVA captions

# Debug 設定
enable_dataset_debug: true  # 顯示每個樣本的詳細資訊
max_samples: null  # 限制樣本數量（null = 處理全部，或設為數字如 10）

# 模型路徑（基礎模型，和 training 一樣）
pretrained_model_name_or_path: "flux_model"
pretrained_adapter_path: "Path_to_pretrained_FLUX_adapter"
transp_vae_path: "ckpt/trans_vae/0008000.pt"

# 預訓練 LoRA（可選，和 training 一樣）
pretrained_lora_dir: "ckpt/pre_trained_LoRA"  # 可選
artplus_lora_dir: "/workspace/ShuoIsAllYouNeed/Yao/final/CLD/ckpt/prism_ft_LoRA"  # 可選

# ⭐ 訓練好的 Checkpoint 路徑（從 training 的 output_dir 中選擇）
# 例如：如果 training 保存到 "FT_on_TAData_ckpt/checkpoint_00200"
# 則設置為：
layer_ckpt: "FT_on_TAData_ckpt/checkpoint_000200"  # Checkpoint 根目錄（用於載入 layer_pe.pth）
lora_ckpt: "FT_on_TAData_ckpt/checkpoint_000200/transformer"  # Transformer LoRA checkpoint
adapter_lora_dir: "FT_on_TAData_ckpt/checkpoint_000200/adapter"  # Adapter LoRA checkpoint

# Inference 參數
cfg: 4.0  # Guidance scale（和 training 一樣）
max_layers: 48  # VAE 最大層數（用於 transparent VAE）
decoder_arch: "vit"  # VAE decoder 架構
pos_embedding: "rope"  # 位置編碼
layer_embedding: "rope"  # Layer 編碼

# 輸出目錄
save_dir: "inference_output"

