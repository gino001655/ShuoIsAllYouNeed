# CLD Inference Configuration for Pipeline-generated data
# This config uses JSON files generated by pipeline_to_cld_infer.py

seed: 42
max_layer_num: 52

# Use pipeline-generated JSON files instead of HuggingFace dataset
use_pipeline_dataset: true

# Path to directory containing JSON files from pipeline_to_cld_infer.py
# This should point to the output_dir from pipeline_config.yaml's cld section
data_dir: "outputs/pipeline_outputs/cld"

# Reduce VRAM by downscaling large inputs at dataset loading time.
# IMPORTANT: CLD has a maximum canvas size limit. Set this to ensure
# final padded size <= 336x336 pixels (21x21 grid units) to avoid tensor dimension errors.
# Recommended for 16GB GPUs. Set to null to disable.
# DISABLED: Commented out to preserve image quality. If tensor dimension errors occur,
# you may need to re-enable this or fix CLD's internal canvas size constraints.
# max_image_side: 336

# Model paths
pretrained_model_name_or_path: "../../checkpoints/flux/FLUX.1-dev"
pretrained_adapter_path: "../../checkpoints/flux/FLUX.1-dev-Controlnet-Inpainting-Alpha"
transp_vae_path: "../../checkpoints/cld/trans_vae/0008000.pt"
pretrained_lora_dir: "../../checkpoints/cld/pre_trained_LoRA"
artplus_lora_dir: "../../checkpoints/cld/prism_ft_LoRA"
lora_ckpt: "../../checkpoints/cld/decouple_LoRA/transformer"
layer_ckpt: "../../checkpoints/cld/decouple_LoRA"
adapter_lora_dir: "../../checkpoints/cld/decouple_LoRA/adapter"

# Output directory for inference results
save_dir: "../../outputs/pipeline_outputs/cld_inference"

# Inference parameters
cfg: 4.0  # Guidance scale
# REMOVED: num_inference_steps - not actually used in pipeline call, pipeline uses default value (28)
skip_fuse_lora: false  # Set to false to enable fuse_lora (faster inference, uses more VRAM)

# VRAM Optimization Parameters
enable_tiled_vae: true  # Enable tiled VAE decoding to reduce VRAM (slower but uses less memory)
vae_tile_size: 512  # Tile size for tiled VAE decoding (only used if enable_tiled_vae=true)

